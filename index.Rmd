---
title: "Practical Machine Learning Project"
author: "ALVARO RODRIGUEZ PEREZ"
date: "3 de octubre de 2017"
output: html_document
---

## INTRODUCTION

This  model consists on a implementation of two models: Regression Tree and Random Forest using a dataset provided no Practical Machine Learning Project.

Project consists in create a classification model to determine a "Classe" Variavel referred to the "how well an exercise training was". 


# IMPORTING DATA

In order to avoid error of importation with missing values. Its importante set na.string option with some characters related to missing values (NA,#DIV/0,"") to help R importation function a correct determination of each type of variavels. 

```{r }
setwd("~/Curso Coursera/Machile Learning/Trabajo")
library(caret)
library(VIM) #missing values maps

training<- read.csv("pml-training.csv", header=TRUE, dec="."
                    ,na.strings = c("NA", "#DIV/0!", ""))

testing <- read.csv("pml-testing.csv", header=TRUE, dec=".",
                    na.strings = c("NA", "#DIV/0!", ""))

```

## Data Cleaning

Testing and Training datasets have a lot of columns with missings values. In order to clean its, I identify columns with missing values in Testing Set and deleted in both Training and Testing Data sets because testing set have more columns with missing values than training set.

Thus, columns 1 to 7 was removed in both datasets because don't have relationship with outcome


```{r }
# cleaning data:

features <- names(testing[,colSums(is.na(testing)) == 0]) # columnas sin NAs
features <- features[-60]


training1<- training[,c(features,"classe")]
training1<- training1[,-(1:7)]


testing1<- testing[,c(features,"problem_id")]
testing1<- testing1[,-(1:7)]


dim(testing1)
dim(training1)


```


## Missing Values Maps

Missing Maps shows no presence of NAs


```{r, echo=FALSE }
# missing analysis:

aggr_plot <- aggr(training1, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.4, gap=3, ylab=c("Histogram of missing data","Pattern"))

aggr_plot <- aggr(testing1, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.4, gap=3, ylab=c("Histogram of missing data","Pattern"))


```

## Data Partition

I created a train and test datasets based in training data set with a proportion of 70/30. Train Set will be for training model proposes and Test Set for Validation proposes

```{r }
# datapartition

set.seed(33)
inTrain<- createDataPartition(training1$classe, p=0.7, list=FALSE)
train<- training1[inTrain,]
test<- training1[-inTrain,]


```

## RPART Model

Next step, creation of a decition tree model: RPART

```{r }
# modelo RPART:

modFit1<- train(classe~., data=train, method= "rpart",
               trControl=trainControl(method="cv", number=10))

modFit1$finalModel

pred1<- predict(modFit1, test)

confusionMatrix(test$classe, pred1)



```

Accuracy (49%) shows a low Performance model explined but a lot of "classes" in dataset. So we try to implement another model ussing Rando Forest.

## Random Forest Model

In order to improve a performance of a model, i set RF Model with Cross Validation ands 10 folds:  

```{r }
# modelo Ramdom forest

modFit2<- train(classe~., data=train, method= "rf"
               , trControl= trainControl(method="cv",number=10))

modFit2$finalModel

pred2<- predict(modFit2, test)

confusionMatrix(test$classe, pred2)

```

Accuracy in confusion matrix Test DataSet increased a lot, obtaining 99,18%.This model is a best classificator.

IN ORDER TO ASWWER THE QUESTION OF A PROJECT: 
THE OUT OF SAMPLE ERROR (GERENALIZATION ERROR) in this case its 0,82% and its obtained like an inverse of accuracy in test dataset (1- 99,18%). 

We can't estimate the out of sample error in testing dataset because don't have the real outcome values. But analising we want expect that both datasets ( training and testing) could not have a diferent estructure or another variavel who can suspect me to detect that a model doesn't works. 

## Predicting values for Testing Set

So, Finally, predicting values for Testing dataSet are obtained as follow:  

```{r }
# predict in Testing Set for evaluation:

resultsRF<-predict(modFit2,testing1)

resultsRF
```


##PRINCIPAL CONCLUSIONS

1) It was submitted a GitHub Repository with both files: RMD and compiled HTML
2) It was builded Two Machine Learning models in order to obtain a best accurray. Decition Tree Model (RPART) don't shows good accuracy, but RF with 10-Folds-Cross-Validation Method obtained 99,18% of accuracy.
3) The out of sample error its estimated in test dataset and its 0.18% and was obtained using crooss validation options included in RF Model described in point (2)


Thanks!.